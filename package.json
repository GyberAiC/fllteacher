{
  "name": "llama-finetuning",
  "version": "1.0.0",
  "type": "module",
  "scripts": {
    "start": "node src/index.js",
    "test": "jest",
    "lint": "eslint src/**/*.js",
    "format": "prettier --write src/**/*.js",
    "validate-data": "node src/scripts/validateData.js",
    "monitor": "node src/scripts/monitorTraining.js"
  },
  "dependencies": {
    "@tensorflow/tfjs": "^4.15.0",
    "@tensorflow/tfjs-node": "^4.15.0",
    "commander": "^11.1.0",
    "dotenv": "^16.4.5",
    "openai": "^4.20.1",
    "winston": "^3.17.0",
    "express": "^4.18.2",
    "socket.io": "^4.7.2"
  },
  "devDependencies": {
    "jest": "^29.7.0",
    "eslint": "^8.56.0",
    "prettier": "^3.1.1",
    "typescript": "^5.3.3",
    "@types/node": "^20.10.5"
  }
}